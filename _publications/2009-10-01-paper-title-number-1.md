---
title: "The loss landscape of deep linear neural networks: a second-order analysis"
collection: publications
permalink: /publication/loss2022
#excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
#date: 2022-01-01
#venue: 'Under review'
paperurl: 'https://arxiv.org/pdf/2107.13289.pdf'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

We study the optimization landscape of deep linear neural networks with the square loss. It is known that, under weak assumptions, there are no spurious local minima and no local maxima. However, the existence and diversity of non-strict saddle points, which can play a role in first-order algorithms' dynamics, have only been lightly studied. We go a step further with a full analysis of the optimization landscape at order 2. We characterize, among all critical points, which are global minimizers, strict saddle points, and non-strict saddle points. We enumerate all the associated critical values. The characterization is simple, involves conditions on the ranks of partial matrix products, and sheds some light on global convergence or implicit regularization that have been proved or observed when optimizing linear neural networks. In passing, we provide an explicit parameterization of the set of all global minimizers and exhibit large sets of strict and non-strict saddle points.

[pdf](https://arxiv.org/pdf/2107.13289.pdf)m

